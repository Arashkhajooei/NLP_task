{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arashkhgit/NLP_task/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqStTXFK-6q3",
        "outputId": "50cd68be-56d4-430e-e164-2d572f32e46d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-29 15:48:49--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-08-29 15:48:49--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-08-29 15:48:50--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2023-08-29 15:51:29 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axRgcbWQ-8sz",
        "outputId": "0d1781df-1a3f-493c-9d17-c740a81eee5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP2a4Ku62sho"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "data = []\n",
        "with open('/content/IMDB Dataset.csv', 'r', encoding='utf-8') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    next(csv_reader)  # Skip header\n",
        "    for row in csv_reader:\n",
        "        review = row[0]\n",
        "        sentiment = row[1]\n",
        "        data.append((review, sentiment))\n",
        "\n",
        "for i in range(len(data)):\n",
        "    review = data[i][0].lower().split()\n",
        "    sentiment = data[i][1]\n",
        "    data[i] = (review, sentiment)\n",
        "\n",
        "# Build vocabulary and convert text to numerical data\n",
        "vocab = {}\n",
        "for review, _ in data:\n",
        "    for word in review:\n",
        "        if word in vocab:\n",
        "            vocab[word] += 1\n",
        "        else:\n",
        "            vocab[word] = 1\n",
        "\n",
        "# Choose a vocabulary size (e.g., 5000 most common words)\n",
        "vocab_size = 1000\n",
        "common_words = sorted(vocab.keys(), key=lambda x: vocab[x], reverse=True)[:vocab_size]\n",
        "word_to_index = {word: idx for idx, word in enumerate(common_words)}\n",
        "\n",
        "numerical_data = []\n",
        "for review, sentiment in data:\n",
        "    numerical_review = [word_to_index[word] for word in review if word in word_to_index]\n",
        "    numerical_data.append((numerical_review, sentiment))\n",
        "\n",
        "# Load pretrained embeddings (GloVe)\n",
        "embedding_path = '/content/glove.6B.50d.txt'  # Change this to match the downloaded file name\n",
        "embedding_dim = 50\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(embedding_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.strip().split()\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "# Create an embedding matrix for the limited vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in word_to_index.items():\n",
        "    if word in embeddings_index:\n",
        "        embedding_matrix[idx] = embeddings_index[word]\n",
        "\n",
        "# Prepare training and testing data\n",
        "split_ratio = 0.8\n",
        "split_idx = int(len(numerical_data) * split_ratio)\n",
        "train_data = numerical_data[:split_idx]\n",
        "test_data = numerical_data[split_idx:]\n",
        "\n",
        "class MiniBatchLSTM:\n",
        "    def __init__(self, vocab_size, hidden_size, output_size):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize LSTM weights and biases here\n",
        "        self.W_f = np.random.randn(hidden_size, vocab_size)\n",
        "        self.W_i = np.random.randn(hidden_size, vocab_size)\n",
        "        self.W_o = np.random.randn(hidden_size, vocab_size)\n",
        "        self.W_c = np.random.randn(hidden_size, vocab_size)\n",
        "\n",
        "        self.b_f = np.random.randn(hidden_size, 1)\n",
        "        self.b_i = np.random.randn(hidden_size, 1)\n",
        "        self.b_o = np.random.randn(hidden_size, 1)\n",
        "        self.b_c = np.random.randn(hidden_size, 1)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        h = np.zeros((self.hidden_size, 1))\n",
        "        c = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "        for t in input_sequence:\n",
        "            f_t = self.sigmoid(np.dot(self.W_f, t) + self.b_f)\n",
        "            i_t = self.sigmoid(np.dot(self.W_i, t) + self.b_i)\n",
        "            o_t = self.sigmoid(np.dot(self.W_o, t) + self.b_o)\n",
        "            g_t = self.tanh(np.dot(self.W_c, t) + self.b_c)\n",
        "\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * self.tanh(c)\n",
        "\n",
        "        return h\n",
        "\n",
        "    def train(self, train_data, num_epochs, learning_rate, batch_size):\n",
        "        for epoch in range(num_epochs):\n",
        "            np.random.shuffle(train_data)\n",
        "            total_loss = 0\n",
        "\n",
        "            for batch_start in range(0, len(train_data), batch_size):\n",
        "                batch_data = train_data[batch_start:batch_start+batch_size]\n",
        "                batch_gradients = []\n",
        "\n",
        "                for review, sentiment in batch_data:\n",
        "                    input_sequence = [np.zeros((self.vocab_size, 1)) for _ in range(self.vocab_size)]\n",
        "                    for idx in review:\n",
        "                        if idx < self.vocab_size:\n",
        "                            input_sequence[idx] = np.ones((self.vocab_size, 1))\n",
        "\n",
        "                    # Forward pass\n",
        "                    predicted_sentiment = self.forward(input_sequence)\n",
        "\n",
        "                    # Loss and gradient calculations\n",
        "                    loss = (predicted_sentiment - int(sentiment)) ** 2\n",
        "                    loss_gradient = 2 * (predicted_sentiment - int(sentiment))\n",
        "                    batch_gradients.append(loss_gradient)\n",
        "\n",
        "                # Backward pass and update weights for the batch\n",
        "                # ...\n",
        "\n",
        "                # Update total loss\n",
        "                total_loss += sum([loss ** 2 for loss in batch_gradients])\n",
        "\n",
        "            print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_data)}\")\n",
        "\n",
        "\n",
        "# Train the MiniBatchLSTM model\n",
        "vocab_size = len(vocab)\n",
        "hidden_size = 32\n",
        "output_size = 1\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "\n",
        "minibatch_lstm = MiniBatchLSTM(vocab_size, hidden_size, output_size)\n",
        "minibatch_lstm.train(train_data, num_epochs, learning_rate, batch_size)\n",
        "\n",
        "# Evaluate and test\n",
        "correct_predictions = 0\n",
        "\n",
        "for review, sentiment in test_data:\n",
        "    input_sequence = [np.zeros((vocab_size, 1)) for _ in range(len(review))]\n",
        "    for idx in review:\n",
        "        input_sequence[idx] = np.ones((vocab_size, 1))\n",
        "\n",
        "    predicted_sentiment = minibatch_lstm.forward(input_sequence)\n",
        "    predicted_sentiment = 1 if predicted_sentiment > 0.5 else 0\n",
        "\n",
        "    if predicted_sentiment == int(sentiment):\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / len(test_data)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8nqLwdGvW7jRTYNV8JOZs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}